{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Sales Data Analysis\n",
    "This notebook outlines the code and results of the ten data analysis tasks for the BS3220 Parallel Programming assignment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, count, max, min, avg, col, rank, to_date, date_format, regexp_extract, row_number, first, last, format_number, substring, year, expr, quarter, round, month\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VehicleSalesCleaning\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"Superstore.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, count, to_date, round, when, isnan\n",
    "\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Sales\", round(\"Sales\", 2))\n",
    "df = df.withColumn(\"Order Date\", to_date(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "\n",
    "df.select(\"Order Date\").show(5, truncate=False)\n",
    "\n",
    "\n",
    "# Extract the year and month from 'Order Date'\n",
    "sales_by_month = df.withColumn(\"Year\", year(\"Order Date\")).withColumn(\"Month\", month(\"Order Date\"))\n",
    "\n",
    "# Group by 'Year' and 'Month' and count the total rows, not just distinct 'Order ID'\n",
    "sales_by_month = sales_by_month.groupBy(\"Year\", \"Month\").agg(\n",
    "    count(\"*\").alias(\"Number of Sales\")  # Count all rows to capture total sales entries\n",
    ").orderBy(\"Number of Sales\")  # Order the results by year and month for better readability\n",
    "\n",
    "# Show the results\n",
    "sales_by_month.show(100)\n",
    "\n",
    "df = df.filter(~col(\"Product Name\").contains('\"'))\n",
    "\n",
    "filtered_count = df.count()\n",
    "print(f\"Filtered row count: {filtered_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Find the total sales for each item, both the number of units and the total price/cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_per_item = df.groupBy(\"Product Name\").agg(\n",
    "    sum(\"Quantity\").alias(\"Total Units\"),\n",
    "    format_number(sum(\"Sales\"), 2).alias(\"Total Sales\")\n",
    ")\n",
    "total_sales_per_item.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Summarise the total sales of all items at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_per_location = df.filter(df[\"Sales\"].isNotNull()) \\\n",
    "    .groupBy(\"City\") \\\n",
    "    .agg(sum(\"Sales\").alias(\"Total Sales\")) \\\n",
    "    .orderBy(\"Total Sales\")\n",
    "\n",
    "# Format the 'Total Sales' column after sorting\n",
    "total_sales_per_location = total_sales_per_location.withColumn(\n",
    "    \"Total Sales\", format_number(\"Total Sales\", 2)\n",
    ")\n",
    "\n",
    "total_sales_per_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: List all products and their combined sales, grouped by their location of sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_by_product_and_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    sum(\"Sales\").alias(\"Sum Sales\")\n",
    ")\n",
    "\n",
    "# Format the 'Sum Sales' and order by 'City' alphabetically\n",
    "sales_by_product_and_location = sales_by_product_and_location.withColumn(\n",
    "    \"Combined Sales\", format_number(\"Sum Sales\", 2)\n",
    ").drop(\"Sum Sales\")  # Optionally drop the unformatted sum column if it's no longer needed\n",
    "\n",
    "# Order the results by 'City'\n",
    "sales_by_product_and_location = sales_by_product_and_location.orderBy(\"City\")\n",
    "\n",
    "# Display the results\n",
    "sales_by_product_and_location.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Show the sales numbers for the item which sold the most units at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"City\").orderBy(col(\"Total Units\").desc())\n",
    "\n",
    "# Group by 'City' and 'Product Name', then aggregate sum of 'Quantity' and sum of 'Sales'\n",
    "best_selling_items_per_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    sum(\"Quantity\").alias(\"Total Units\"),\n",
    "    sum(\"Sales\").alias(\"Total Sales\")\n",
    ").withColumn(\"rank\", rank().over(windowSpec))  # Apply ranking within each city\n",
    "\n",
    "# Filter to get only the top selling item per location and then drop the 'rank' column\n",
    "best_selling_items_per_location = best_selling_items_per_location.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show the results\n",
    "best_selling_items_per_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: List all items that were sold within two months of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, month, year, sum, round\n",
    "\n",
    "df = df.withColumn(\"Sales\", round(\"Sales\", 2))\n",
    "df = df.withColumn(\"Order Date\", to_date(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "\n",
    "# Filter and aggregate sales for January 2014\n",
    "jan_2014_sales = df.filter((month(\"Order Date\") == 1) & (year(\"Order Date\") == 2014))\n",
    "jan_2014_total_sales = jan_2014_sales.groupBy(\"Product Name\").agg(sum(\"Sales\").alias(\"Total Sales\"))\n",
    "total_jan_2014 = jan_2014_sales.agg(sum(\"Sales\").alias(\"Total Sales for January 2014\")).collect()[0][0]\n",
    "\n",
    "# Filter and aggregate sales for December 2014\n",
    "dec_2014_sales = df.filter((month(\"Order Date\") == 12) & (year(\"Order Date\") == 2014))\n",
    "dec_2014_total_sales = dec_2014_sales.groupBy(\"Product Name\").agg(sum(\"Sales\").alias(\"Total Sales\"))\n",
    "total_dec_2014 = dec_2014_sales.agg(sum(\"Sales\").alias(\"Total Sales for December 2014\")).collect()[0][0]\n",
    "\n",
    "# Print the total sales for January 2014\n",
    "print(f\"Total Sales for January 2014: {total_jan_2014:.2f}\")\n",
    "\n",
    "print(\"Sales Details for January 2014:\")\n",
    "jan_2014_total_sales.show(truncate=False)\n",
    "\n",
    "# Print the total sales for December 2014\n",
    "print(f\"Total Sales for December 2014: {total_dec_2014:.2f}\")\n",
    "\n",
    "print(\"Sales Details for December 2014:\")\n",
    "dec_2014_total_sales.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Identify the item which has the lowest overall sales, both for the dataset as a whole and for each sales location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "lowest_sales_overall = df.groupBy(\"Product Name\").agg(\n",
    "    count(\"*\").alias(\"Number of Sales\")\n",
    ").orderBy(\"Number of Sales\").limit(1)\n",
    "\n",
    "# Calculate the lowest number of sales occurrences per location\n",
    "lowest_sales_by_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    count(\"*\").alias(\"Number of Sales\")\n",
    ")\n",
    "\n",
    "# Define a window specification that partitions by 'City' and orders by 'Number of Sales' and 'Product Name'\n",
    "windowSpec = Window.partitionBy(\"City\").orderBy(col(\"Number of Sales\"), col(\"Product Name\"))\n",
    "\n",
    "# Apply a window function to rank products by number of sales per city, and to sort ties alphabetically by product name\n",
    "ranked_sales_by_location = lowest_sales_by_location.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "# Filter to get only the product with the lowest sales (rank 1) per city\n",
    "lowest_sales_details_by_location = ranked_sales_by_location.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Number of Sales\")\n",
    "\n",
    "print(\"Item with the Lowest Number of Sales Occurrences Overall:\")\n",
    "lowest_sales_overall.show(truncate=False)\n",
    "\n",
    "print(\"Item with the Lowest Number of Sales Occurrences per Location:\")\n",
    "lowest_sales_details_by_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Find the most expensive and least expensive item for each location where sales occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, min, col, rank\n",
    "\n",
    "# Assuming 'df' has already been prepared with correct data types and necessary columns\n",
    "# Group by City and Product Name to find the most and least expensive sales\n",
    "expensiveCheapPerLocation = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    max(\"Sales\").alias(\"Most Expensive Sale\"), \n",
    "    min(\"Sales\").alias(\"Least Expensive Sale\")\n",
    ")\n",
    "\n",
    "# Define window specifications for both the most expensive and the least expensive items\n",
    "windowSpecMost = Window.partitionBy(\"City\").orderBy(col(\"Most Expensive Sale\").desc())\n",
    "windowSpecLeast = Window.partitionBy(\"City\").orderBy(col(\"Least Expensive Sale\"))\n",
    "\n",
    "# Apply the window function to rank the products within each city for most expensive\n",
    "mostExpensivePerLocation = expensiveCheapPerLocation.withColumn(\"rank\", rank().over(windowSpecMost))\n",
    "mostExpensivePerLocation = mostExpensivePerLocation.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Most Expensive Sale\")\n",
    "\n",
    "# Apply the window function to rank the products within each city for least expensive\n",
    "leastExpensivePerLocation = expensiveCheapPerLocation.withColumn(\"rank\", rank().over(windowSpecLeast))\n",
    "leastExpensivePerLocation = leastExpensivePerLocation.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Least Expensive Sale\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Most Expensive Items per Location:\")\n",
    "mostExpensivePerLocation.show()\n",
    "\n",
    "print(\"Least Expensive Items per Location:\")\n",
    "leastExpensivePerLocation.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Calculate the average cost of an item at each location within your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, round\n",
    "\n",
    "# Group by 'City' and calculate the average 'Sales', then round to 2 decimal places\n",
    "average_cost_per_location = df.groupBy(\"City\").agg(\n",
    "    round(avg(\"Sales\"), 2).alias(\"Average Sales\")\n",
    ").orderBy(\"City\")  # Ordering by 'City' for a better presentation\n",
    "\n",
    "# Show the results\n",
    "print(\"Average Cost of an Item at Each Location:\")\n",
    "average_cost_per_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Based on your individual dataset, create a set of variables which can be used as broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Complete one other query to analyse the data, based on your individual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, lit, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import to_date, quarter, year\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Order Date\", to_date(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "df = df.withColumn(\"Year\", year(\"Order Date\"))\n",
    "df = df.withColumn(\"Quarter\", quarter(\"Order Date\"))\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(IntegerType()))\n",
    "df = df.withColumn(\"Discount\", col(\"Discount\").cast(\"double\"))\n",
    "\n",
    "# Filter data for the year 2013 only\n",
    "df_2013 = df.filter(col(\"Year\") == 2013)\n",
    "\n",
    "# Split data into training (Q1 to Q3) and testing (Q4) sets\n",
    "train_data = df_2013.filter(df_2013[\"Quarter\"].isin([1, 2, 3]))\n",
    "test_data = df_2013.filter(df_2013[\"Quarter\"] == 4)\n",
    "\n",
    "# Aggregate the total sales from Q1-Q3 for training\n",
    "total_sales_q1_q3 = train_data.agg(spark_sum(\"Sales\").alias(\"Total Sales Q1-Q3\"))\n",
    "total_sales_q4 = test_data.agg(spark_sum(\"Sales\").alias(\"Actual Total Sales Q4\"))\n",
    "\n",
    "# Prepare features using VectorAssembler even for a constant feature\n",
    "assembler = VectorAssembler(inputCols=[], outputCol=\"features\")\n",
    "train_data_features = assembler.transform(total_sales_q1_q3).select(\"features\", \"Total Sales Q1-Q3\")\n",
    "test_data_features = assembler.transform(total_sales_q4).select(\"features\", \"Actual Total Sales Q4\")\n",
    "\n",
    "# Define the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Total Sales Q1-Q3\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Fit the model on aggregated training data\n",
    "model = pipeline.fit(train_data_features)\n",
    "\n",
    "# Predict on the aggregated test data\n",
    "predictions = model.transform(test_data_features)\n",
    "\n",
    "# Show predicted vs actual\n",
    "predictions.select(\"Actual Total Sales Q4\", \"prediction\").show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"Actual Total Sales Q4\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on total sales prediction: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 31780.4\n",
      "+----+-------+-----------------+------------------+\n",
      "|Year|Quarter|       TotalSales|        prediction|\n",
      "+----+-------+-----------------+------------------+\n",
      "|2014|      4|270048.4300000002|238268.05966878505|\n",
      "+----+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import quarter, to_timestamp, avg, col \n",
    "df = df.withColumn(\"Order Date\", to_timestamp(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "df = df.withColumn(\"Quarter\", quarter(\"Order Date\"))\n",
    "\n",
    "# Grouping data by year, quarter, and calculating the sum of sales, quantity, and profit\n",
    "quarterly_data = df.groupBy(\"Year\", \"Quarter\").agg(\n",
    "    sum(\"Sales\").alias(\"TotalSales\"),\n",
    "    sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "    sum(\"Profit\").alias(\"TotalProfit\")\n",
    ")\n",
    "\n",
    "# Split data into training (Q1-Q3) and testing (Q4)\n",
    "train_data = quarterly_data.filter(col(\"Quarter\") < 4)\n",
    "test_data = quarterly_data.filter((col(\"Quarter\") == 4) & (col(\"Year\") == 2014))\n",
    "\n",
    "# Preparing the assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"TotalQuantity\", \"TotalProfit\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Transforming data\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Defining the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"TotalSales\")\n",
    "\n",
    "# Training the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Making predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluating the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"Year\", \"Quarter\", \"TotalSales\", \"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
