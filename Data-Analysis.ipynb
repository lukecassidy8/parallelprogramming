{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cd0534",
   "metadata": {},
   "source": [
    "# Vehicle Sales Data Analysis\n",
    "This notebook outlines the code and results of the ten data analysis tasks for the BS3220 Parallel Programming assignment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba7f75",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e92837ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, count, max, min, avg, col, rank, to_date, date_format, regexp_extract, row_number, first, last, format_number, substring, year, expr, quarter\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VehicleSalesCleaning\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"car_prices.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72818841",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e0106bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "|year| make|              model|      trim| body|transmission|              vin|state|condition|odometer|color|interior|              seller|  mmr|sellingprice|            saledate|\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "|2015|  Kia|            Sorento|        LX|  SUV|   automatic|5xyktca69fg566472|   ca|        5|   16639|white|   black|kia motors americ...|20500|       21500|Tue Dec 16 2014 1...|\n",
      "|2015|  Kia|            Sorento|        LX|  SUV|   automatic|5xyktca69fg561319|   ca|        5|    9393|white|   beige|kia motors americ...|20800|       21500|Tue Dec 16 2014 1...|\n",
      "|2014|  BMW|           3 Series|328i SULEV|Sedan|   automatic|wba3c1c51ek116351|   ca|       45|    1331| gray|   black|financial service...|31900|       30000|Thu Jan 15 2015 0...|\n",
      "|2015|Volvo|                S60|        T5|Sedan|   automatic|yv1612tb4f1310987|   ca|       41|   14282|white|   black|volvo na rep/worl...|27500|       27750|Thu Jan 29 2015 0...|\n",
      "|2014|  BMW|6 Series Gran Coupe|      650i|Sedan|   automatic|wba6b2c57ed129731|   ca|       43|    2641| gray|   black|financial service...|66000|       67000|Thu Dec 18 2014 1...|\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extracted Date Part:\n",
      "+--------------------+---------------+\n",
      "|            saledate|      date_part|\n",
      "+--------------------+---------------+\n",
      "|Tue Dec 16 2014 1...|Tue Dec 16 2014|\n",
      "|Tue Dec 16 2014 1...|Tue Dec 16 2014|\n",
      "|Thu Jan 15 2015 0...|Thu Jan 15 2015|\n",
      "|Thu Jan 29 2015 0...|Thu Jan 29 2015|\n",
      "|Thu Dec 18 2014 1...|Thu Dec 18 2014|\n",
      "+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Transformed Data with Quarters:\n",
      "+--------------------+------------------+---------+----------+\n",
      "|            saledate|formatted_saledate|sale_year|sale_month|\n",
      "+--------------------+------------------+---------+----------+\n",
      "|Tue Dec 16 2014 1...|          12162014|     2014|    122014|\n",
      "|Tue Dec 16 2014 1...|          12162014|     2014|    122014|\n",
      "|Thu Jan 15 2015 0...|          01152015|     2015|    012015|\n",
      "|Thu Jan 29 2015 0...|          01292015|     2015|    012015|\n",
      "|Thu Dec 18 2014 1...|          12182014|     2014|    122014|\n",
      "+--------------------+------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data:\")\n",
    "df.show(5)\n",
    "\n",
    "df = df.filter(df.sellingprice != 1)\n",
    "\n",
    "df = df.dropna(subset=[\"make\", \"model\", \"trim\", \"body\", \"vin\", \"state\", \"condition\", \"odometer\", \"color\", \"interior\", \"seller\", \"mmr\"])\n",
    "\n",
    "# Extract the date part excluding time and timezone using regular expressions\n",
    "df = df.withColumn(\"date_part\", regexp_extract(col(\"saledate\"), r\"(\\w+\\s\\w+\\s\\d+\\s\\d+)\", 1))\n",
    "\n",
    "print(\"Extracted Date Part:\")\n",
    "df.select(\"saledate\", \"date_part\").show(5)\n",
    "\n",
    "# Convert extracted string to date and format it to MMddyyyy\n",
    "df = df.withColumn(\"formatted_saledate\", date_format(to_date(col(\"date_part\"), \"EEE MMM dd yyyy\"), \"MMddyyyy\"))\n",
    "df = df.withColumn(\"sale_year\", expr(\"substring(formatted_saledate, 5, 4)\"))\n",
    "\n",
    "df = df.withColumn(\"sale_month\", expr(\"substring(formatted_saledate, 1, 2) || substring(formatted_saledate, 5, 4)\"))\n",
    "\n",
    "\n",
    "print(\"Transformed Data with Quarters:\")\n",
    "df.select(\"saledate\", \"formatted_saledate\", \"sale_year\", \"sale_month\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3f06c",
   "metadata": {},
   "source": [
    "### Task 1: Find the total sales for each item, both the number of units and the total price/cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1003f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupBy(\"make\", \"model\").agg(\n",
    "#     count(\"vin\").alias(\"units_sold\"),\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\")\n",
    "# ).orderBy(col(\"units_sold\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdb4b8",
   "metadata": {},
   "source": [
    "### Task 2: Summarise the total sales of all items at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2de7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesPerState = df.groupBy(\"state\").agg(\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\"), count(\"vin\").alias(\"total_sales\")\n",
    "# ).orderBy(\"total_revenue\", ascending=False)\n",
    "\n",
    "# salesPerState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec6cd",
   "metadata": {},
   "source": [
    "### Task 3: List all products and their combined sales, grouped by their location of sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cb7e1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sales_by_state_and_product = df.groupBy(\"state\", \"make\", \"model\").agg(\n",
    "#     count(\"vin\").alias(\"total_sales\"), sum(\"sellingprice\").alias(\"totalrevenue\")\n",
    "# ).orderBy(\"state\")\n",
    "\n",
    "# combined_sales_by_state_and_product.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72929101",
   "metadata": {},
   "source": [
    "### Task 4: Show the sales numbers for the item which sold the most units at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "216b679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_sales_by_state = df.groupBy(\"state\", \"make\", \"model\", \"trim\").agg(\n",
    "#     count(\"vin\").alias(\"units_sold\"),\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\")\n",
    "# )\n",
    "\n",
    "# windowSpec = Window.partitionBy(\"state\").orderBy(col(\"units_sold\").desc())\n",
    "\n",
    "# # Rank items within each state and select top item\n",
    "# top_item_by_state = item_sales_by_state.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "# top_item_by_state.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf3904",
   "metadata": {},
   "source": [
    "### Task 5: List all items that were sold within two months of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bf6eb3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|     make|              model|            saledate|\n",
      "+---------+-------------------+--------------------+\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|      BMW|6 Series Gran Coupe|Thu Dec 18 2014 1...|\n",
      "|   Nissan|             Altima|Tue Dec 30 2014 1...|\n",
      "|      BMW|                 M5|Wed Dec 17 2014 1...|\n",
      "|Chevrolet|              Cruze|Tue Dec 16 2014 1...|\n",
      "|     Audi|                 A4|Thu Dec 18 2014 1...|\n",
      "|     Audi|                 A6|Tue Dec 16 2014 1...|\n",
      "|      Kia|             Optima|Tue Dec 16 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|   Nissan|             Altima|Tue Dec 23 2014 1...|\n",
      "|     Audi|                 Q5|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|             Camaro|Tue Dec 30 2014 1...|\n",
      "|      BMW|           6 Series|Wed Dec 17 2014 1...|\n",
      "|     Audi|                 A3|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|             Camaro|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|              Cruze|Tue Dec 30 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|     Audi|                 S5|Thu Dec 18 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----------+\n",
      "|month|total_sales|\n",
      "+-----+-----------+\n",
      "|   06|      97866|\n",
      "|   12|      40559|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter((col(\"formatted_saledate\").substr(1,2) == \"12\") | (col(\"formatted_saledate\").substr(1,2)==\"06\"))\n",
    "\n",
    "salesByMonth = df_filtered.groupBy(col(\"formatted_saledate\").substr(1,2).alias(\"month\")).agg(count(\"vin\").alias(\"total_sales\")).orderBy(\"month\")\n",
    "\n",
    "df_filtered.select(\"make\", \"model\", \"saledate\").show()\n",
    "salesByMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae7bee",
   "metadata": {},
   "source": [
    "### Task 6: Identify the item which has the lowest overall sales, both for the dataset as a whole and for each sales location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57686fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesPerCar = df.groupBy(\"make\", \"model\").agg(\n",
    "#     count(\"*\").alias(\"unitsSold\")\n",
    "# )\n",
    "# salesPerCarPerState = df.groupBy(\"state\", \"make\", \"model\").agg(\n",
    "#     count(\"*\").alias(\"unitsSold\")\n",
    "# )\n",
    "# lowestCarSale = salesPerCar.orderBy(\"unitsSold\").first()\n",
    "\n",
    "# windowSpec = Window.partitionBy(\"state\").orderBy(\"unitsSold\")\n",
    "# lowestCarSaleByState = salesPerCarPerState.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col(\"row_number\") == 1)\n",
    "\n",
    "# print(\"Lowest selling car:\")\n",
    "# print(lowestCarSale)\n",
    "# print(\"Lowest Selling Car by State:\")\n",
    "# lowestCarSaleByState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e448a",
   "metadata": {},
   "source": [
    "### Task 7: Find the most expensive and least expensive item for each location where sales occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2d595f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowExpensive = Window.partitionBy(\"state\").orderBy(col(\"sellingprice\").desc())\n",
    "# windowCheap = Window.partitionBy(\"state\").orderBy(col(\"sellingprice\").asc())\n",
    "\n",
    "# dfRanked = df.withColumn(\"rank_desc\", row_number().over(windowExpensive)).withColumn(\"rank_asc\", row_number().over(windowCheap))\n",
    "\n",
    "# mostExpensive = dfRanked.filter(col(\"rank_desc\") == 1).select(\n",
    "#     \"state\", \"make\", \"model\", \"sellingprice\"\n",
    "# )\n",
    "# leastExpensive = dfRanked.filter(col(\"rank_asc\") == 1).select(\n",
    "#     \"state\", \"make\", \"model\", \"sellingprice\"\n",
    "# )\n",
    "\n",
    "# print(\"Most Expensive Car Sale by State:\")\n",
    "# mostExpensive.show(10)\n",
    "# print(\"Least Expensive Car Sale by State:\")\n",
    "# leastExpensive.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4441a74",
   "metadata": {},
   "source": [
    "### Task 8: Calculate the average cost of an item at each location within your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "089015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averagePriceByState = df.groupBy(\"state\").agg(\n",
    "#     format_number(avg(\"sellingprice\"), 2).alias(\"average_selling_price\")\n",
    "# )\n",
    "# averagePriceByState = averagePriceByState.orderBy(\"state\")\n",
    "\n",
    "# print(\"Average Selling Price by State:\")\n",
    "# averagePriceByState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3e524",
   "metadata": {},
   "source": [
    "### Task 9: Based on your individual dataset, create a set of variables which can be used as broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0b7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949d7bcb",
   "metadata": {},
   "source": [
    "### Task 10: Complete one other query to analyse the data, based on your individual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4a6ff1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateAverage = df.groupBy(\"state\").agg(avg(\"sellingprice\").alias(\"averageRevenuePerSale\")).orderBy(col(\"averageRevenuePerSale\").desc())\n",
    "\n",
    "# #Formatting 2DP for revenue after ordering as was causing table to sort lowest first, not highest first for some reason\n",
    "# formattedBestStateForAverageRevenue = stateAverage.withColumn(\n",
    "#     \"formattedAverageRevenue\", format_number(col(\"averageRevenuePerSale\"), 2)\n",
    "# )\n",
    "# print(\"Best average revenue per sale:\")\n",
    "# formattedBestStateForAverageRevenue.select(\"state\", \"formattedAverageRevenue\").show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9e690",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a6516d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|sale_month| sales|\n",
      "+----------+------+\n",
      "|     22015|158880|\n",
      "|     42015|  1402|\n",
      "|    122014| 40559|\n",
      "|     32015| 45144|\n",
      "|     12015|137091|\n",
      "|     22014|     1|\n",
      "|     72015|  1273|\n",
      "|     12014|   201|\n",
      "|     52015| 51228|\n",
      "|     62015| 97866|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|sale_month| sales|\n",
      "+----------+------+\n",
      "|     22015|158880|\n",
      "|     42015|  1402|\n",
      "|     32015| 45144|\n",
      "|     12015|137091|\n",
      "|     22014|     1|\n",
      "+----------+------+\n",
      "\n",
      "+----------+-----+\n",
      "|sale_month|sales|\n",
      "+----------+-----+\n",
      "|     72015| 1273|\n",
      "|     52015|51228|\n",
      "|     62015|97866|\n",
      "+----------+-----+\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 143043.06381536784\n",
      "+---------+-----+-------------------+\n",
      "| features|sales|         prediction|\n",
      "+---------+-----+-------------------+\n",
      "|[72015.0]| 1273|-131544.07812622408|\n",
      "|[52015.0]|51228| -44567.20492731448|\n",
      "|[62015.0]|97866|  -88055.6415267693|\n",
      "+---------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Aggregate sales by sale_month, assuming each row in df represents a sale\n",
    "sales_data = df.groupBy(\"sale_month\").agg(count(\"vin\").alias(\"sales\"))\n",
    "\n",
    "# Convert 'sale_month' to numeric type if it's not already\n",
    "sales_data = sales_data.withColumn(\"sale_month\", sales_data[\"sale_month\"].cast(\"integer\"))\n",
    "\n",
    "# Filter data for training and testing\n",
    "train_data = sales_data.filter(col(\"sale_month\").between(12015, 42015))\n",
    "test_data = sales_data.filter(col(\"sale_month\").between(52015, 72015))\n",
    "\n",
    "\n",
    "# Use VectorAssembler to combine feature columns\n",
    "assembler = VectorAssembler(inputCols=[\"sale_month\"], outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data).select(\"features\", \"sales\")\n",
    "test_data = assembler.transform(test_data).select(\"features\", \"sales\")\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"sales\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n",
    "\n",
    "# Show the predictions\n",
    "predictions.select(\"features\", \"sales\", \"prediction\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
