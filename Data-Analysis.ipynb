{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cd0534",
   "metadata": {},
   "source": [
    "# Vehicle Sales Data Analysis\n",
    "This notebook outlines the code and results of the ten data analysis tasks for the BS3220 Parallel Programming assignment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba7f75",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e92837ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, count, max, min, avg, col, rank, to_date, date_format, regexp_extract, row_number, first, last, format_number, substring, year, expr, quarter\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VehicleSalesCleaning\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"car_prices.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72818841",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e0106bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "|year| make|              model|      trim| body|transmission|              vin|state|condition|odometer|color|interior|              seller|  mmr|sellingprice|            saledate|\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "|2015|  Kia|            Sorento|        LX|  SUV|   automatic|5xyktca69fg566472|   ca|        5|   16639|white|   black|kia motors americ...|20500|       21500|Tue Dec 16 2014 1...|\n",
      "|2015|  Kia|            Sorento|        LX|  SUV|   automatic|5xyktca69fg561319|   ca|        5|    9393|white|   beige|kia motors americ...|20800|       21500|Tue Dec 16 2014 1...|\n",
      "|2014|  BMW|           3 Series|328i SULEV|Sedan|   automatic|wba3c1c51ek116351|   ca|       45|    1331| gray|   black|financial service...|31900|       30000|Thu Jan 15 2015 0...|\n",
      "|2015|Volvo|                S60|        T5|Sedan|   automatic|yv1612tb4f1310987|   ca|       41|   14282|white|   black|volvo na rep/worl...|27500|       27750|Thu Jan 29 2015 0...|\n",
      "|2014|  BMW|6 Series Gran Coupe|      650i|Sedan|   automatic|wba6b2c57ed129731|   ca|       43|    2641| gray|   black|financial service...|66000|       67000|Thu Dec 18 2014 1...|\n",
      "+----+-----+-------------------+----------+-----+------------+-----------------+-----+---------+--------+-----+--------+--------------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Extracted Date Part:\n",
      "+--------------------+---------------+\n",
      "|            saledate|      date_part|\n",
      "+--------------------+---------------+\n",
      "|Tue Dec 16 2014 1...|Tue Dec 16 2014|\n",
      "|Tue Dec 16 2014 1...|Tue Dec 16 2014|\n",
      "|Thu Jan 15 2015 0...|Thu Jan 15 2015|\n",
      "|Thu Jan 29 2015 0...|Thu Jan 29 2015|\n",
      "|Thu Dec 18 2014 1...|Thu Dec 18 2014|\n",
      "+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Transformed Data with Quarters:\n",
      "+--------------------+------------------+---------+------------+\n",
      "|            saledate|formatted_saledate|sale_year|sale_quarter|\n",
      "+--------------------+------------------+---------+------------+\n",
      "|Tue Dec 16 2014 1...|          12162014|     2014|           4|\n",
      "|Tue Dec 16 2014 1...|          12162014|     2014|           4|\n",
      "|Thu Jan 15 2015 0...|          01152015|     2015|           1|\n",
      "|Thu Jan 29 2015 0...|          01292015|     2015|           1|\n",
      "|Thu Dec 18 2014 1...|          12182014|     2014|           4|\n",
      "+--------------------+------------------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data:\")\n",
    "df.show(5)\n",
    "\n",
    "df = df.filter(df.sellingprice != 1)\n",
    "\n",
    "df = df.dropna(subset=[\"make\", \"model\", \"trim\", \"body\", \"vin\", \"state\", \"condition\", \"odometer\", \"color\", \"interior\", \"seller\", \"mmr\"])\n",
    "\n",
    "# Extract the date part excluding time and timezone using regular expressions\n",
    "df = df.withColumn(\"date_part\", regexp_extract(col(\"saledate\"), r\"(\\w+\\s\\w+\\s\\d+\\s\\d+)\", 1))\n",
    "\n",
    "print(\"Extracted Date Part:\")\n",
    "df.select(\"saledate\", \"date_part\").show(5)\n",
    "\n",
    "# Convert extracted string to date and format it to MMddyyyy\n",
    "df = df.withColumn(\"formatted_saledate\", date_format(to_date(col(\"date_part\"), \"EEE MMM dd yyyy\"), \"MMddyyyy\"))\n",
    "df = df.withColumn(\"sale_year\", expr(\"substring(formatted_saledate, 5, 4)\"))\n",
    "\n",
    "df = df.withColumn(\"sale_month\", expr(\"substring(formatted_saledate, 1, 2)\"))\n",
    "\n",
    "\n",
    "print(\"Transformed Data with Quarters:\")\n",
    "df.select(\"saledate\", \"formatted_saledate\", \"sale_year\", \"sale_month\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3f06c",
   "metadata": {},
   "source": [
    "### Task 1: Find the total sales for each item, both the number of units and the total price/cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1003f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupBy(\"make\", \"model\").agg(\n",
    "#     count(\"vin\").alias(\"units_sold\"),\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\")\n",
    "# ).orderBy(col(\"units_sold\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdb4b8",
   "metadata": {},
   "source": [
    "### Task 2: Summarise the total sales of all items at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b2de7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesPerState = df.groupBy(\"state\").agg(\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\"), count(\"vin\").alias(\"total_sales\")\n",
    "# ).orderBy(\"total_revenue\", ascending=False)\n",
    "\n",
    "# salesPerState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec6cd",
   "metadata": {},
   "source": [
    "### Task 3: List all products and their combined sales, grouped by their location of sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cb7e1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_sales_by_state_and_product = df.groupBy(\"state\", \"make\", \"model\").agg(\n",
    "#     count(\"vin\").alias(\"total_sales\"), sum(\"sellingprice\").alias(\"totalrevenue\")\n",
    "# ).orderBy(\"state\")\n",
    "\n",
    "# combined_sales_by_state_and_product.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72929101",
   "metadata": {},
   "source": [
    "### Task 4: Show the sales numbers for the item which sold the most units at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "216b679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_sales_by_state = df.groupBy(\"state\", \"make\", \"model\", \"trim\").agg(\n",
    "#     count(\"vin\").alias(\"units_sold\"),\n",
    "#     sum(\"sellingprice\").alias(\"total_revenue\")\n",
    "# )\n",
    "\n",
    "# windowSpec = Window.partitionBy(\"state\").orderBy(col(\"units_sold\").desc())\n",
    "\n",
    "# # Rank items within each state and select top item\n",
    "# top_item_by_state = item_sales_by_state.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "# top_item_by_state.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf3904",
   "metadata": {},
   "source": [
    "### Task 5: List all items that were sold within two months of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bf6eb3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+\n",
      "|     make|              model|            saledate|\n",
      "+---------+-------------------+--------------------+\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|      BMW|6 Series Gran Coupe|Thu Dec 18 2014 1...|\n",
      "|   Nissan|             Altima|Tue Dec 30 2014 1...|\n",
      "|      BMW|                 M5|Wed Dec 17 2014 1...|\n",
      "|Chevrolet|              Cruze|Tue Dec 16 2014 1...|\n",
      "|     Audi|                 A4|Thu Dec 18 2014 1...|\n",
      "|     Audi|                 A6|Tue Dec 16 2014 1...|\n",
      "|      Kia|             Optima|Tue Dec 16 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|   Nissan|             Altima|Tue Dec 23 2014 1...|\n",
      "|     Audi|                 Q5|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|             Camaro|Tue Dec 30 2014 1...|\n",
      "|      BMW|           6 Series|Wed Dec 17 2014 1...|\n",
      "|     Audi|                 A3|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|             Camaro|Thu Dec 18 2014 1...|\n",
      "|Chevrolet|              Cruze|Tue Dec 30 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "|     Audi|                 S5|Thu Dec 18 2014 1...|\n",
      "|      Kia|            Sorento|Tue Dec 16 2014 1...|\n",
      "+---------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----------+\n",
      "|month|total_sales|\n",
      "+-----+-----------+\n",
      "|   06|      97866|\n",
      "|   12|      40559|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter((col(\"formatted_saledate\").substr(1,2) == \"12\") | (col(\"formatted_saledate\").substr(1,2)==\"06\"))\n",
    "\n",
    "salesByMonth = df_filtered.groupBy(col(\"formatted_saledate\").substr(1,2).alias(\"month\")).agg(count(\"vin\").alias(\"total_sales\")).orderBy(\"month\")\n",
    "\n",
    "df_filtered.select(\"make\", \"model\", \"saledate\").show()\n",
    "salesByMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae7bee",
   "metadata": {},
   "source": [
    "### Task 6: Identify the item which has the lowest overall sales, both for the dataset as a whole and for each sales location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "57686fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salesPerCar = df.groupBy(\"make\", \"model\").agg(\n",
    "#     count(\"*\").alias(\"unitsSold\")\n",
    "# )\n",
    "# salesPerCarPerState = df.groupBy(\"state\", \"make\", \"model\").agg(\n",
    "#     count(\"*\").alias(\"unitsSold\")\n",
    "# )\n",
    "# lowestCarSale = salesPerCar.orderBy(\"unitsSold\").first()\n",
    "\n",
    "# windowSpec = Window.partitionBy(\"state\").orderBy(\"unitsSold\")\n",
    "# lowestCarSaleByState = salesPerCarPerState.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col(\"row_number\") == 1)\n",
    "\n",
    "# print(\"Lowest selling car:\")\n",
    "# print(lowestCarSale)\n",
    "# print(\"Lowest Selling Car by State:\")\n",
    "# lowestCarSaleByState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e448a",
   "metadata": {},
   "source": [
    "### Task 7: Find the most expensive and least expensive item for each location where sales occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2d595f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowExpensive = Window.partitionBy(\"state\").orderBy(col(\"sellingprice\").desc())\n",
    "# windowCheap = Window.partitionBy(\"state\").orderBy(col(\"sellingprice\").asc())\n",
    "\n",
    "# dfRanked = df.withColumn(\"rank_desc\", row_number().over(windowExpensive)).withColumn(\"rank_asc\", row_number().over(windowCheap))\n",
    "\n",
    "# mostExpensive = dfRanked.filter(col(\"rank_desc\") == 1).select(\n",
    "#     \"state\", \"make\", \"model\", \"sellingprice\"\n",
    "# )\n",
    "# leastExpensive = dfRanked.filter(col(\"rank_asc\") == 1).select(\n",
    "#     \"state\", \"make\", \"model\", \"sellingprice\"\n",
    "# )\n",
    "\n",
    "# print(\"Most Expensive Car Sale by State:\")\n",
    "# mostExpensive.show(10)\n",
    "# print(\"Least Expensive Car Sale by State:\")\n",
    "# leastExpensive.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4441a74",
   "metadata": {},
   "source": [
    "### Task 8: Calculate the average cost of an item at each location within your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "089015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averagePriceByState = df.groupBy(\"state\").agg(\n",
    "#     format_number(avg(\"sellingprice\"), 2).alias(\"average_selling_price\")\n",
    "# )\n",
    "# averagePriceByState = averagePriceByState.orderBy(\"state\")\n",
    "\n",
    "# print(\"Average Selling Price by State:\")\n",
    "# averagePriceByState.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3e524",
   "metadata": {},
   "source": [
    "### Task 9: Based on your individual dataset, create a set of variables which can be used as broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0b7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949d7bcb",
   "metadata": {},
   "source": [
    "### Task 10: Complete one other query to analyse the data, based on your individual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4a6ff1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateAverage = df.groupBy(\"state\").agg(avg(\"sellingprice\").alias(\"averageRevenuePerSale\")).orderBy(col(\"averageRevenuePerSale\").desc())\n",
    "\n",
    "# #Formatting 2DP for revenue after ordering as was causing table to sort lowest first, not highest first for some reason\n",
    "# formattedBestStateForAverageRevenue = stateAverage.withColumn(\n",
    "#     \"formattedAverageRevenue\", format_number(col(\"averageRevenuePerSale\"), 2)\n",
    "# )\n",
    "# print(\"Best average revenue per sale:\")\n",
    "# formattedBestStateForAverageRevenue.select(\"state\", \"formattedAverageRevenue\").show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9e690",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a6516d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Sales Data for 2015:\n",
      "+-----+-----+\n",
      "|month|sales|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|features|sales|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3301.fit.\n: java.lang.AssertionError: assertion failed: Training dataset is empty.\r\n\tat scala.Predef$.assert(Predef.scala:223)\r\n\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\r\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\r\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\r\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Linear Regression model setup\u001b[39;00m\n\u001b[0;32m     30\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\lukem\\Documents\\Uni\\Parallel\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3301.fit.\n: java.lang.AssertionError: assertion failed: Training dataset is empty.\r\n\tat scala.Predef$.assert(Predef.scala:223)\r\n\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\r\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\r\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\r\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
