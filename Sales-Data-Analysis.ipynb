{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Sales Data Analysis\n",
    "This notebook outlines the code and results of the ten data analysis tasks for the BS3220 Parallel Programming assignment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Discount Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "\n",
    "df = spark.read.csv(\"Superstore.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Order Date|\n",
      "+----------+\n",
      "|2013-11-09|\n",
      "|2013-11-09|\n",
      "|2013-06-13|\n",
      "|2012-10-11|\n",
      "|2012-10-11|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Filtered row count: 9183\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, year, count, to_date, round\n",
    "\n",
    "df = df.withColumn(\"Sales\", round(\"Sales\", 2))\n",
    "df = df.withColumn(\"Order Date\", to_date(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "\n",
    "df.select(\"Order Date\").show(5, truncate=False)\n",
    "df = df.filter(~col(\"Product Name\").contains('\"'))\n",
    "\n",
    "filtered_count = df.count()\n",
    "print(f\"Filtered row count: {filtered_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Find the total sales for each item, both the number of units and the total price/cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+-----------+-----------+\n",
      "|Product Name                                                                     |Total Units|Total Sales|\n",
      "+---------------------------------------------------------------------------------+-----------+-----------+\n",
      "|GBC ProClick 150 Presentation Binding System                                     |22.0       |3,222.99   |\n",
      "|Wasp CCD Handheld Bar Code Reader                                                |3.0        |336.51     |\n",
      "|Wilson Jones 14 Line Acrylic Coated Pressboard Data Binders                      |23.0       |100.39     |\n",
      "|Aastra 6757i CT Wireless VoIP phone                                              |16.0       |2,929.98   |\n",
      "|Global Armless Task Chair, Royal Blue                                            |28.0       |1,396.44   |\n",
      "|Sortfiler Multipurpose Personal File Organizer, Black                            |24.0       |483.42     |\n",
      "|Bretford �Just In Time� Height-Adjustable Multi-Task Work Tables                 |17.0       |5,634.90   |\n",
      "|Avery Durable Binders                                                            |50.0       |87.26      |\n",
      "|Xerox 212                                                                        |25.0       |141.27     |\n",
      "|Office Star - Contemporary Swivel Chair with Padded Adjustable Arms and Flex Back|32.0       |3,989.73   |\n",
      "|Maxell 4.7GB DVD+R 5/Pack                                                        |13.0       |11.69      |\n",
      "|Acco Economy Flexible Poly Round Ring Binder                                     |6.0        |11.48      |\n",
      "|Fellowes Recycled Storage Drawers                                                |12.0       |1,287.95   |\n",
      "|Safco Commercial Shelving                                                        |11.0       |483.71     |\n",
      "|Xerox 1901                                                                       |4.0        |16.90      |\n",
      "|Plantronics Voyager Pro HD - Bluetooth Headset                                   |25.0       |1,221.82   |\n",
      "|Dana Swing-Arm Lamps                                                             |15.0       |108.93     |\n",
      "|Cisco IP�Phone�7961G VoIP�phone�- Dark gray                                      |12.0       |1,386.69   |\n",
      "|GBC VeloBinder Strips                                                            |20.0       |52.99      |\n",
      "|Okidata C610n Printer                                                            |5.0        |2,011.90   |\n",
      "+---------------------------------------------------------------------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_per_item = df.groupBy(\"Product Name\").agg(\n",
    "    sum(\"Quantity\").alias(\"Total Units\"),\n",
    "    format_number(sum(\"Sales\"), 2).alias(\"Total Sales\")\n",
    ")\n",
    "total_sales_per_item.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Summarise the total sales of all items at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|City         |Total Sales|\n",
      "+-------------+-----------+\n",
      "|New York City|241,000.45 |\n",
      "|Los Angeles  |168,539.72 |\n",
      "|Seattle      |113,898.95 |\n",
      "|San Francisco|109,699.89 |\n",
      "|Philadelphia |104,549.69 |\n",
      "|Houston      |63,716.90  |\n",
      "|Chicago      |47,004.49  |\n",
      "|San Diego    |46,052.02  |\n",
      "|Jacksonville |44,289.18  |\n",
      "|Detroit      |42,276.50  |\n",
      "|Springfield  |40,729.08  |\n",
      "|Columbus     |36,474.45  |\n",
      "|Newark       |27,832.00  |\n",
      "|Lafayette    |25,001.83  |\n",
      "|Columbia     |23,649.72  |\n",
      "|Burlington   |21,623.33  |\n",
      "|Jackson      |21,315.85  |\n",
      "|San Antonio  |21,164.86  |\n",
      "|Dallas       |19,529.04  |\n",
      "|Richmond     |17,849.57  |\n",
      "+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_per_location = df.filter(df[\"Sales\"].isNotNull()) \\\n",
    "    .groupBy(\"City\") \\\n",
    "    .agg(sum(\"Sales\").alias(\"Total Sales\")) \\\n",
    "    .orderBy(col(\"Total Sales\").desc()) \n",
    "\n",
    "# Format the column after sorting\n",
    "total_sales_per_location = total_sales_per_location.withColumn(\n",
    "    \"Total Sales\", format_number(\"Total Sales\", 2)\n",
    ")\n",
    "\n",
    "total_sales_per_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: List all products and their combined sales, grouped by their location of sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------------------------+--------------+\n",
      "|City    |Product Name                                                   |Combined Sales|\n",
      "+--------+---------------------------------------------------------------+--------------+\n",
      "|Aberdeen|Acme Titanium Bonded Scissors                                  |25.50         |\n",
      "|Abilene |Hoover Commercial Lightweight Upright Vacuum                   |1.39          |\n",
      "|Akron   |Newell 315                                                     |14.35         |\n",
      "|Akron   |Cisco IP Phone 7961G-GE VoIP phone                             |259.90        |\n",
      "|Akron   |GBC Recycled VeloBinder Covers                                 |25.56         |\n",
      "|Akron   |Deflect-o Glass Clear Studded Chair Mats                       |149.23        |\n",
      "|Akron   |Belkin F8E887 USB Wired Ergonomic Keyboard                     |71.98         |\n",
      "|Akron   |Southworth 25% Cotton Linen-Finish Paper & Envelopes           |21.74         |\n",
      "|Akron   |Google Nexus 5                                                 |323.98        |\n",
      "|Akron   |SAFCO Commercial Wire Shelving, Black                          |221.02        |\n",
      "|Akron   |Xerox 1949                                                     |15.94         |\n",
      "|Akron   |Wilson Jones Active Use Binders                                |4.37          |\n",
      "|Akron   |Plantronics Voyager Pro Legend                                 |247.19        |\n",
      "|Akron   |GBC Standard Therm-A-Bind Covers                               |14.95         |\n",
      "|Akron   |Deluxe Rollaway Locking File with Drawer                       |665.41        |\n",
      "|Akron   |Logitech G500s Laser Gaming Mouse with Adjustable Weight Tuning|279.96        |\n",
      "|Akron   |Acco Hanging Data Binders                                      |2.29          |\n",
      "|Akron   |Acco Expandable Hanging Binders                                |5.74          |\n",
      "|Akron   |OIC Colored Binder Clips, Assorted Sizes                       |17.18         |\n",
      "|Akron   |Xerox 1906                                                     |85.06         |\n",
      "+--------+---------------------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_product_and_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    sum(\"Sales\").alias(\"Sum Sales\")\n",
    ")\n",
    "\n",
    "sales_by_product_and_location = sales_by_product_and_location.withColumn(\n",
    "    \"Combined Sales\", format_number(\"Sum Sales\", 2)\n",
    ").drop(\"Sum Sales\")\n",
    "\n",
    "# Ordering the results by City\n",
    "sales_by_product_and_location = sales_by_product_and_location.orderBy(\"City\")\n",
    "\n",
    "sales_by_product_and_location.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Show the sales numbers for the item which sold the most units at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+-----------+\n",
      "|        City|        Product Name|Total Units|Total Sales|\n",
      "+------------+--------------------+-----------+-----------+\n",
      "|    Aberdeen|Acme Titanium Bon...|        3.0|       25.5|\n",
      "|     Abilene|Hoover Commercial...|        2.0|       1.39|\n",
      "|       Akron|OIC Colored Binde...|        6.0|      17.18|\n",
      "| Albuquerque|OtterBox Commuter...|        8.0|     140.74|\n",
      "| Albuquerque|SimpliFile Person...|        8.0|       90.8|\n",
      "|  Alexandria|DAX Wood Document...|       14.0|     192.22|\n",
      "|       Allen|           Xerox 198|        4.0|      15.94|\n",
      "|       Allen|GBC Imprintable C...|        4.0|       8.78|\n",
      "|   Allentown|SAFCO Commercial ...|        6.0|     663.07|\n",
      "|     Altoona|Eldon Spacemaker ...|        6.0|      16.03|\n",
      "|    Amarillo|HON 5400 Series T...|        5.0|    2453.43|\n",
      "|    Amarillo|           Xerox 212|        5.0|      25.92|\n",
      "|     Anaheim|GBC Recycled Velo...|        9.0|     122.69|\n",
      "|     Anaheim|Carina Mini Syste...|        9.0|     998.82|\n",
      "|     Andover|Situations Contou...|        5.0|      354.9|\n",
      "|   Ann Arbor|             Staples|        7.0|      57.96|\n",
      "|     Antioch|           Xerox 216|        3.0|      19.44|\n",
      "|      Apopka|GBC Prepunched Pa...|        6.0|      27.02|\n",
      "|      Apopka|DataProducts Ampl...|        6.0|     129.89|\n",
      "|Apple Valley|Peel & Seel Recyc...|        5.0|       57.9|\n",
      "+------------+--------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"City\").orderBy(col(\"Total Units\").desc())\n",
    "\n",
    "best_selling_items_per_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    sum(\"Quantity\").alias(\"Total Units\"),\n",
    "    sum(\"Sales\").alias(\"Total Sales\")\n",
    ").withColumn(\"rank\", rank().over(windowSpec)) \n",
    "\n",
    "best_selling_items_per_location = best_selling_items_per_location.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "best_selling_items_per_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: List all items that were sold within two months of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales for January 2014: 44048.21\n",
      "Sales Details for January 2014:\n",
      "+----------------------------------------------------+-----------+\n",
      "|Product Name                                        |Total Sales|\n",
      "+----------------------------------------------------+-----------+\n",
      "|GBC ProClick 150 Presentation Binding System        |2022.27    |\n",
      "|Avery Durable Binders                               |14.4       |\n",
      "|Xerox 212                                           |25.92      |\n",
      "|Flat Face Poster Frame                              |37.68      |\n",
      "|Bagged Rubber Bands                                 |2.52       |\n",
      "|GBC Instant Report Kit                              |38.82      |\n",
      "|Avanti 4.4 Cu. Ft. Refrigerator                     |542.94     |\n",
      "|Office Star - Contemporary Task Swivel Chair        |310.74     |\n",
      "|Quartet Omega Colored Chalk, 12/Pack                |9.34       |\n",
      "|Airmail Envelopes                                   |268.58     |\n",
      "|Strathmore Photo Mount Cards                        |21.7       |\n",
      "|Xerox 1997                                          |12.96      |\n",
      "|Xerox 192                                           |6.48       |\n",
      "|Samsung Galaxy Mega 6.3                             |2939.93    |\n",
      "|Poly String Tie Envelopes                           |2.04       |\n",
      "|DXL Angle-View Binders with Locking Rings by Samsill|7.71       |\n",
      "|Tennsco Industrial Shelving                         |156.51     |\n",
      "|Novimex Swivel Fabric Task Chair                    |120.78     |\n",
      "|Acco D-Ring Binder w/DublLock                       |4.28       |\n",
      "|Eldon Fold 'N Roll Cart System                      |153.78     |\n",
      "+----------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total Sales for December 2014: 84766.08\n",
      "Sales Details for December 2014:\n",
      "+---------------------------------------------------------------------------------+-----------+\n",
      "|Product Name                                                                     |Total Sales|\n",
      "+---------------------------------------------------------------------------------+-----------+\n",
      "|Office Star - Contemporary Swivel Chair with Padded Adjustable Arms and Flex Back|1141.94    |\n",
      "|Bretford �Just In Time� Height-Adjustable Multi-Task Work Tables                 |1669.6     |\n",
      "|Acco Economy Flexible Poly Round Ring Binder                                     |3.13       |\n",
      "|Safco Commercial Shelving                                                        |37.21      |\n",
      "|Okidata C610n Printer                                                            |649.0      |\n",
      "|Avery 473                                                                        |72.45      |\n",
      "|Microsoft Wireless Mobile Mouse 4000                                             |199.95     |\n",
      "|Xerox 1884                                                                       |143.86     |\n",
      "|Wirebound Message Book, 4 per Page                                               |38.01      |\n",
      "|Eureka The Boss Lite 10-Amp Upright Vacuum, Blue                                 |320.64     |\n",
      "|Novimex High-Tech Fabric Mesh Task Chair                                         |113.57     |\n",
      "|Bagged Rubber Bands                                                              |3.02       |\n",
      "|Global Comet Stacking Armless Chair                                              |897.15     |\n",
      "|Logitech G105 Gaming Keyboard                                                    |178.11     |\n",
      "|Avery 492                                                                        |5.76       |\n",
      "|Belkin SportFit Armband For iPhone 5s/5c, Fuchsia                                |62.96      |\n",
      "|DAX Two-Tone Silver Metal Document Frame                                         |16.19      |\n",
      "|Premier Elliptical Ring Binder, Black                                            |45.66      |\n",
      "|Eldon Gobal File Keepers                                                         |15.14      |\n",
      "|Microsoft Natural Ergonomic Keyboard 4000                                        |149.95     |\n",
      "+---------------------------------------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Sales\", round(\"Sales\", 2))\n",
    "df = df.withColumn(\"Order Date\", to_date(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "\n",
    "jan_2014_sales = df.filter((month(\"Order Date\") == 1) & (year(\"Order Date\") == 2014))\n",
    "jan_2014_total_sales = jan_2014_sales.groupBy(\"Product Name\").agg(sum(\"Sales\").alias(\"Total Sales\"))\n",
    "total_jan_2014 = jan_2014_sales.agg(sum(\"Sales\").alias(\"Total Sales for January 2014\")).collect()[0][0]\n",
    "\n",
    "dec_2014_sales = df.filter((month(\"Order Date\") == 12) & (year(\"Order Date\") == 2014))\n",
    "dec_2014_total_sales = dec_2014_sales.groupBy(\"Product Name\").agg(sum(\"Sales\").alias(\"Total Sales\"))\n",
    "total_dec_2014 = dec_2014_sales.agg(sum(\"Sales\").alias(\"Total Sales for December 2014\")).collect()[0][0]\n",
    "\n",
    "print(f\"Total Sales for January 2014: {total_jan_2014:.2f}\")\n",
    "\n",
    "print(\"Sales Details for January 2014:\")\n",
    "jan_2014_total_sales.show(truncate=False)\n",
    "\n",
    "print(f\"Total Sales for December 2014: {total_dec_2014:.2f}\")\n",
    "\n",
    "print(\"Sales Details for December 2014:\")\n",
    "dec_2014_total_sales.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Identify the item which has the lowest overall sales, both for the dataset as a whole and for each sales location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item with the Lowest Number of Sales Occurrences Overall:\n",
      "+---------------------------------+---------------+\n",
      "|Product Name                     |Number of Sales|\n",
      "+---------------------------------+---------------+\n",
      "|Wasp CCD Handheld Bar Code Reader|1              |\n",
      "+---------------------------------+---------------+\n",
      "\n",
      "Item with the Lowest Number of Sales Occurrences per Location:\n",
      "+-----------------+-------------------------------------------------------+---------------+\n",
      "|City             |Product Name                                           |Number of Sales|\n",
      "+-----------------+-------------------------------------------------------+---------------+\n",
      "|Aberdeen         |Acme Titanium Bonded Scissors                          |1              |\n",
      "|Abilene          |Hoover Commercial Lightweight Upright Vacuum           |1              |\n",
      "|Akron            |Acco Expandable Hanging Binders                        |1              |\n",
      "|Albuquerque      |AT&T TR1909W                                           |1              |\n",
      "|Alexandria       |AT&T CL82213                                           |1              |\n",
      "|Allen            |Chromcraft Round Conference Tables                     |1              |\n",
      "|Allentown        |12 Colored Short Pencils                               |1              |\n",
      "|Altoona          |Eldon Spacemaker Box, Quick-Snap Lid, Clear            |1              |\n",
      "|Amarillo         |Bush Mission Pointe Library                            |1              |\n",
      "|Anaheim          |Acme Box Cutter Scissors                               |1              |\n",
      "|Andover          |GBC Binding covers                                     |1              |\n",
      "|Ann Arbor        |Belkin 6 Outlet Metallic Surge Strip                   |1              |\n",
      "|Antioch          |Xerox 216                                              |1              |\n",
      "|Apopka           |DataProducts Ampli Magnifier Task Lamp, Black,         |1              |\n",
      "|Apple Valley     |Atlantic Metals Mobile 4-Shelf Bookcases, Custom Colors|1              |\n",
      "|Appleton         |Avery 481                                              |1              |\n",
      "|Arlington        |AT&T CL82213                                           |1              |\n",
      "|Arlington Heights|Newell 332                                             |1              |\n",
      "|Arvada           |Avery 509                                              |1              |\n",
      "|Asheville        |Avery Durable Slant Ring Binders                       |1              |\n",
      "+-----------------+-------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowest_sales_overall = df.groupBy(\"Product Name\").agg(\n",
    "    count(\"*\").alias(\"Number of Sales\")\n",
    ").orderBy(\"Number of Sales\").limit(1)\n",
    "\n",
    "lowest_sales_by_location = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    count(\"*\").alias(\"Number of Sales\")\n",
    ")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"City\").orderBy(col(\"Number of Sales\"), col(\"Product Name\"))\n",
    "\n",
    "ranked_sales_by_location = lowest_sales_by_location.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "lowest_sales_details_by_location = ranked_sales_by_location.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Number of Sales\")\n",
    "\n",
    "print(\"Item with the Lowest Number of Sales Occurrences Overall:\")\n",
    "lowest_sales_overall.show(truncate=False)\n",
    "\n",
    "print(\"Item with the Lowest Number of Sales Occurrences per Location:\")\n",
    "lowest_sales_details_by_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Find the most expensive and least expensive item for each location where sales occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Expensive Items per Location:\n",
      "+-----------------+------------------------------------------------------------------+-------------------+\n",
      "|City             |Product Name                                                      |Most Expensive Sale|\n",
      "+-----------------+------------------------------------------------------------------+-------------------+\n",
      "|Aberdeen         |Acme Titanium Bonded Scissors                                     |25.5               |\n",
      "|Abilene          |Hoover Commercial Lightweight Upright Vacuum                      |1.39               |\n",
      "|Akron            |Deluxe Rollaway Locking File with Drawer                          |665.41             |\n",
      "|Albuquerque      |WD My Passport Ultra 2TB Portable External Hard Drive             |595.0              |\n",
      "|Alexandria       |Martin Yale Chadless Opener Electric Letter Opener                |4164.05            |\n",
      "|Allen            |Chromcraft Round Conference Tables                                |244.01             |\n",
      "|Allentown        |SAFCO Commercial Wire Shelving, Black                             |663.07             |\n",
      "|Altoona          |Eldon Spacemaker Box, Quick-Snap Lid, Clear                       |16.03              |\n",
      "|Amarillo         |HON 5400 Series Task Chairs for Big and Tall                      |2453.43            |\n",
      "|Anaheim          |Tennsco 16-Compartment Lockers with Coat Rack                     |1295.78            |\n",
      "|Andover          |Situations Contoured Folding Chairs, 4/Set                        |354.9              |\n",
      "|Ann Arbor        |Logitech G19 Programmable Gaming Keyboard                         |619.95             |\n",
      "|Antioch          |Xerox 216                                                         |19.44              |\n",
      "|Apopka           |Sanyo 2.5 Cubic Foot Mid-Size Office Refrigerators                |671.54             |\n",
      "|Apple Valley     |Atlantic Metals Mobile 4-Shelf Bookcases, Custom Colors           |1194.17            |\n",
      "|Appleton         |Plantronics CS510 - Over-the-Head monaural Wireless Headset System|1649.75            |\n",
      "|Arlington        |Cisco SPA301                                                      |1871.88            |\n",
      "|Arlington Heights|Newell 332                                                        |14.11              |\n",
      "|Arvada           |Hon 4070 Series Pagoda Armless Upholstered Stacking Chairs        |466.77             |\n",
      "|Asheville        |Cisco Unified IP Phone 7945G VoIP phone                           |1363.96            |\n",
      "+-----------------+------------------------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Least Expensive Items per Location:\n",
      "+-----------------+-----------------------------------------------------------------------------------------------+--------------------+\n",
      "|City             |Product Name                                                                                   |Least Expensive Sale|\n",
      "+-----------------+-----------------------------------------------------------------------------------------------+--------------------+\n",
      "|Aberdeen         |Acme Titanium Bonded Scissors                                                                  |25.5                |\n",
      "|Abilene          |Hoover Commercial Lightweight Upright Vacuum                                                   |1.39                |\n",
      "|Akron            |Acco Hanging Data Binders                                                                      |2.29                |\n",
      "|Albuquerque      |Xerox 1953                                                                                     |4.28                |\n",
      "|Alexandria       |Eldon Image Series Black Desk Accessories                                                      |12.42               |\n",
      "|Allen            |GBC Imprintable Covers                                                                         |8.78                |\n",
      "|Allentown        |Staples                                                                                        |3.49                |\n",
      "|Altoona          |Eldon Spacemaker Box, Quick-Snap Lid, Clear                                                    |16.03               |\n",
      "|Amarillo         |Staples                                                                                        |19.65               |\n",
      "|Anaheim          |Newell 323                                                                                     |3.36                |\n",
      "|Andover          |Personal Folder Holder, Ebony                                                                  |11.21               |\n",
      "|Ann Arbor        |KLD Oscar II Style Snap-on Ultra Thin Side Flip Synthetic Leather Cover Case for HTC One HTC M7|29.16               |\n",
      "|Antioch          |Xerox 216                                                                                      |19.44               |\n",
      "|Apopka           |Pressboard Hanging Data Binders for Unburst Sheets                                             |2.95                |\n",
      "|Apple Valley     |Newell 346                                                                                     |8.64                |\n",
      "|Appleton         |Avery 481                                                                                      |21.56               |\n",
      "|Arlington        |Computer Printout Index Tabs                                                                   |1.34                |\n",
      "|Arlington Heights|Newell 332                                                                                     |14.11               |\n",
      "|Arvada           |Avery 509                                                                                      |6.26                |\n",
      "|Asheville        |Newell 350                                                                                     |7.87                |\n",
      "+-----------------+-----------------------------------------------------------------------------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expensiveCheapPerLocation = df.groupBy(\"City\", \"Product Name\").agg(\n",
    "    max(\"Sales\").alias(\"Most Expensive Sale\"), \n",
    "    min(\"Sales\").alias(\"Least Expensive Sale\")\n",
    ")\n",
    "\n",
    "windowSpecMost = Window.partitionBy(\"City\").orderBy(col(\"Most Expensive Sale\").desc())\n",
    "windowSpecLeast = Window.partitionBy(\"City\").orderBy(col(\"Least Expensive Sale\"))\n",
    "\n",
    "mostExpensivePerLocation = expensiveCheapPerLocation.withColumn(\"rank\", rank().over(windowSpecMost))\n",
    "mostExpensivePerLocation = mostExpensivePerLocation.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Most Expensive Sale\")\n",
    "\n",
    "leastExpensivePerLocation = expensiveCheapPerLocation.withColumn(\"rank\", rank().over(windowSpecLeast))\n",
    "leastExpensivePerLocation = leastExpensivePerLocation.filter(col(\"rank\") == 1).select(\n",
    "    \"City\", \"Product Name\", \"Least Expensive Sale\")\n",
    "\n",
    "print(\"Most Expensive Items per Location:\")\n",
    "mostExpensivePerLocation.show(truncate=False)\n",
    "\n",
    "print(\"Least Expensive Items per Location:\")\n",
    "leastExpensivePerLocation.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Calculate the average cost of an item at each location within your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cost of an Item at Each Location:\n",
      "+-----------------+-------------+\n",
      "|City             |Average Sales|\n",
      "+-----------------+-------------+\n",
      "|Aberdeen         |25.5         |\n",
      "|Abilene          |1.39         |\n",
      "|Akron            |135.92       |\n",
      "|Albuquerque      |158.58       |\n",
      "|Alexandria       |391.68       |\n",
      "|Allen            |72.55        |\n",
      "|Allentown        |121.89       |\n",
      "|Altoona          |16.03        |\n",
      "|Amarillo         |416.66       |\n",
      "|Anaheim          |337.12       |\n",
      "|Andover          |108.96       |\n",
      "|Ann Arbor        |177.85       |\n",
      "|Antioch          |19.44        |\n",
      "|Apopka           |142.6        |\n",
      "|Apple Valley     |228.11       |\n",
      "|Appleton         |835.66       |\n",
      "|Arlington        |203.4        |\n",
      "|Arlington Heights|14.11        |\n",
      "|Arvada           |125.85       |\n",
      "|Asheville        |210.77       |\n",
      "+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_cost_per_location = df.groupBy(\"City\").agg(\n",
    "    round(avg(\"Sales\"), 2).alias(\"Average Sales\")\n",
    ").orderBy(\"City\") \n",
    "\n",
    "print(\"Average Cost of an Item at Each Location:\")\n",
    "average_cost_per_location.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Based on your individual dataset, create a set of variables which can be used as broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o368.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 35) (UK-LGWCT2F3.groupinfra.com executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\parallelprogramming\\Sales-Data-Analysis.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mDiscount Type\u001b[39m\u001b[39m\"\u001b[39m, discount_type_udf(col(\u001b[39m\"\u001b[39m\u001b[39mDiscount\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m discount_summary \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupBy(\u001b[39m\"\u001b[39m\u001b[39mDiscount Type\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39magg(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     count(\u001b[39m\"\u001b[39m\u001b[39mOrder ID\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mNumber of Orders\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mround\u001b[39m(\u001b[39msum\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSales\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mTotal Sales\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mround\u001b[39m(avg(\u001b[39m\"\u001b[39m\u001b[39mSales\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mAverage Sales\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mTotal Sales\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luke.cassidy/Documents/University%20of%20Winchester/Year%203/Parallel%20Programming/summative-work/parallelprogramming/Sales-Data-Analysis.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m discount_summary\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\.venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, n: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m, truncate: Union[\u001b[39mbool\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, vertical: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[39m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\.venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\luke.cassidy\\Documents\\University of Winchester\\Year 3\\Parallel Programming\\summative-work\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o368.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 35) (UK-LGWCT2F3.groupinfra.com executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Discount\", col(\"Discount\").cast(\"double\"))\n",
    "\n",
    "discount_categories = {\n",
    "    0.0: \"No Discount\",\n",
    "    0.1: \"Student Discount\",\n",
    "    0.15: \"Seasonal Discount\",\n",
    "    0.2: \"Promotional Discount\",\n",
    "    0.3: \"Blue Light Discount - Basic\",\n",
    "    0.32: \"Blue Light Discount - Enhanced\",\n",
    "    0.4: \"Frequent Shopper Discount\",\n",
    "    0.45: \"Affiliate Discount\",\n",
    "    0.5: \"Standard Employee Discount\",\n",
    "    0.6: \"Senior Employee Discount\",\n",
    "    0.7: \"Management Discount\",\n",
    "    0.8: \"Executive Employee Discount\"\n",
    "}\n",
    "\n",
    "# Broadcast the discount categories dictionary\n",
    "broadcast_discount_categories = spark.sparkContext.broadcast(discount_categories)\n",
    "\n",
    "def get_discount_type(discount):\n",
    "    return broadcast_discount_categories.value.get(discount, \"Unknown Discount\")\n",
    "\n",
    "discount_type_udf = udf(get_discount_type, StringType())\n",
    "\n",
    "df = df.withColumn(\"Discount Type\", discount_type_udf(col(\"Discount\")))\n",
    "\n",
    "discount_summary = df.groupBy(\"Discount Type\").agg(\n",
    "    count(\"Order ID\").alias(\"Number of Orders\"),\n",
    "    round(sum(\"Sales\"), 2).alias(\"Total Sales\"),\n",
    "    round(avg(\"Sales\"), 2).alias(\"Average Sales\")\n",
    ").orderBy(\"Total Sales\", ascending=False)\n",
    "\n",
    "discount_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Complete one other query to analyse the data, based on your individual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Days with the Most Orders:\n",
      "+----------+-----------+\n",
      "|Order Date|Order Count|\n",
      "+----------+-----------+\n",
      "|2013-09-06|         17|\n",
      "|2014-12-03|         16|\n",
      "|2014-11-20|         15|\n",
      "|2014-09-05|         15|\n",
      "|2014-11-25|         15|\n",
      "+----------+-----------+\n",
      "\n",
      "Top 5 Days with the Least Orders:\n",
      "+----------+-----------+\n",
      "|Order Date|Order Count|\n",
      "+----------+-----------+\n",
      "|2012-07-14|          1|\n",
      "|2011-07-02|          1|\n",
      "|2012-05-02|          1|\n",
      "|2012-11-11|          1|\n",
      "|2011-08-30|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Order Date\", to_date(col(\"Order Date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "order_counts = df.groupBy(\"Order Date\").agg(countDistinct(\"Order ID\").alias(\"Order Count\"))\n",
    "\n",
    "top_days = order_counts.orderBy(desc(\"Order Count\")).limit(5)\n",
    "\n",
    "least_days = order_counts.orderBy(\"Order Count\").limit(5)\n",
    "\n",
    "print(\"Top 5 Days with the Most Orders:\")\n",
    "top_days.show()\n",
    "\n",
    "print(\"Top 5 Days with the Least Orders:\")\n",
    "least_days.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 31780.4\n",
      "+----+-------+-----------------+------------------+\n",
      "|Year|Quarter|       TotalSales|        prediction|\n",
      "+----+-------+-----------------+------------------+\n",
      "|2014|      4|270048.4300000002|238268.05966878505|\n",
      "+----+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Order Date\", to_timestamp(\"Order Date\", \"dd/MM/yyyy\"))\n",
    "df = df.withColumn(\"Year\", year(\"Order Date\"))\n",
    "df = df.withColumn(\"Quarter\", quarter(\"Order Date\"))\n",
    "\n",
    "quarterly_data = df.groupBy(\"Year\", \"Quarter\").agg(\n",
    "    sum(\"Sales\").alias(\"TotalSales\"),\n",
    "    sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "    sum(\"Profit\").alias(\"TotalProfit\")\n",
    ")\n",
    "\n",
    "# Split data into training (Q1-Q3) and testing (Q4)\n",
    "train_data = quarterly_data.filter(col(\"Quarter\") < 4)\n",
    "test_data = quarterly_data.filter((col(\"Quarter\") == 4) & (col(\"Year\") == 2014))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"TotalQuantity\", \"TotalProfit\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"TotalSales\")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "predictions.select(\"Year\", \"Quarter\", \"TotalSales\", \"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
